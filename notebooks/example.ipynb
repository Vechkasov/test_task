{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611766a8-b19e-478f-83f1-8003e2385cad",
   "metadata": {},
   "source": [
    "## Переменные окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46b0b52-0bd4-4a26-9f4c-0205a8effd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переменные окружения для PostgreSQL:\n",
      "\tPOSTGRES_HOST: postgres\n",
      "\tPOSTGRES_PASSWORD: airflow\n",
      "\tPOSTGRES_PORT: 5432\n",
      "\tPOSTGRES_USER: airflow\n",
      "\tPOSTGRES_DB: airflow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Переменные окружения для PostgreSQL:\")\n",
    "for key, value in os.environ.items():\n",
    "    if 'POSTGRES' in key: print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0701aa-cd57-4f78-9eef-3a9e99dbbcfe",
   "metadata": {},
   "source": [
    "## Подключение к PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c72742-41de-4d94-bfa2-3eca6088e418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "os.environ.get('PATH')\n",
    "def connect_to_postgres(host=os.environ.get('POSTGRES_HOST'), \n",
    "                        port=os.environ.get('POSTGRES_PORT'), \n",
    "                        database=os.environ.get('POSTGRES_DB'), \n",
    "                        user=os.environ.get('POSTGRES_USER'), \n",
    "                        password=os.environ.get('POSTGRES_PASSWORD')):\n",
    "    \"\"\" Подключение к PostgreSQL \"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            database=database,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        return None\n",
    "\n",
    "conn = connect_to_postgres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50988d9f-7be3-46c8-a0a5-17397f85bd24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postgres', 'airflow']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_databases(connection):\n",
    "    \"\"\" Получение списка всех баз данных \"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT datname FROM pg_database WHERE datistemplate = false;\")\n",
    "        databases = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        return [db[0] for db in databases]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении списка баз данных: {e}\")\n",
    "        return []\n",
    "\n",
    "databases = get_all_databases(conn)\n",
    "databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dbd8d3-6687-4624-b547-ae0a7c13a482",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mart', 'pg_toast', 'public', 'raw', 'stage']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_schemas(connection, database):\n",
    "    \"\"\" Получение списка схем в указанной базе данных \"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT schema_name \n",
    "            FROM information_schema.schemata \n",
    "            WHERE schema_name NOT IN ('pg_catalog', 'information_schema')\n",
    "            ORDER BY schema_name;\n",
    "        \"\"\")\n",
    "        schemas = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        return [schema[0] for schema in schemas]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении списка схем: {e}\")\n",
    "        return []\n",
    "\n",
    "schemas = get_schemas(conn, databases[0])\n",
    "schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e9a06d-0567-47e8-a61b-49690e1ccd69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mart - []\n",
      "pg_toast - []\n",
      "public - [('ab_permission',), ('ab_permission_view',), ('ab_permission_view_role',), ('ab_register_user',), ('ab_role',), ('ab_user',), ('ab_user_role',), ('ab_view_menu',), ('alembic_version',), ('callback_request',), ('connection',), ('dag',), ('dag_code',), ('dag_owner_attributes',), ('dag_pickle',), ('dag_priority_parsing_request',), ('dag_run',), ('dag_run_note',), ('dag_schedule_dataset_alias_reference',), ('dag_schedule_dataset_reference',), ('dag_tag',), ('dag_warning',), ('dagrun_dataset_event',), ('dataset',), ('dataset_alias',), ('dataset_alias_dataset',), ('dataset_alias_dataset_event',), ('dataset_dag_run_queue',), ('dataset_event',), ('import_error',), ('job',), ('log',), ('log_template',), ('rendered_task_instance_fields',), ('serialized_dag',), ('session',), ('sla_miss',), ('slot_pool',), ('task_fail',), ('task_instance',), ('task_instance_history',), ('task_instance_note',), ('task_map',), ('task_outlet_dataset_reference',), ('task_reschedule',), ('trigger',), ('variable',), ('xcom',)]\n",
      "raw - []\n",
      "stage - []\n"
     ]
    }
   ],
   "source": [
    "def get_tables_in_schema(connection, schema_name='public'):\n",
    "    \"\"\" Получение списка таблиц в указанной схеме \"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        query = sql.SQL(\"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = %s\n",
    "            ORDER BY table_name;\n",
    "        \"\"\")\n",
    "        cursor.execute(query, (schema_name,))\n",
    "        tables = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении списка таблиц: {e}\")\n",
    "        return []\n",
    "\n",
    "for schema in schemas:\n",
    "    print(f'{schema} - {get_tables_in_schema(conn, schema)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cdd033-923c-4f72-af46-2550defff7e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users - [('id', 'integer', 'YES', None, None), ('first_name', 'text', 'YES', None, None), ('last_name', 'text', 'YES', None, None), ('age', 'integer', 'YES', None, None), ('created_date', 'date', 'YES', None, None)]\n"
     ]
    }
   ],
   "source": [
    "def get_table_columns(connection, schema_name, table_name):\n",
    "    \"\"\" Получение информации о колонках таблицы \"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        query = sql.SQL(\"\"\"\n",
    "            SELECT \n",
    "                column_name, \n",
    "                data_type, \n",
    "                is_nullable,\n",
    "                character_maximum_length,\n",
    "                column_default\n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = %s AND table_name = %s\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\")\n",
    "        cursor.execute(query, (schema_name, table_name))\n",
    "        columns = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении информации о колонках: {e}\")\n",
    "        return []\n",
    "\n",
    "schema, table = 'public', 'ab_permission' \n",
    "# schema, table = 'raw', 'users'\n",
    "print(f'{table} - {get_table_columns(conn, schema, table)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678cb55-d199-415a-92c0-9c874536b6c0",
   "metadata": {},
   "source": [
    "## PySpark примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91ded38e-3012-402c-ae1e-ee84ef954518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_2020.csv  raw_2021.csv  raw_2022.csv  raw_2023.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jovyan/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "875c0bec-0381-4673-86d9-b45abebb7aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|     Country/Region|      Province/State|1/22/20|1/23/20|1/24/20|1/25/20|1/26/20|1/27/20|1/28/20|1/29/20|\n",
      "+-------------------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|        Afghanistan|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|            Albania|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|            Algeria|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|            Andorra|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|             Angola|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|         Antarctica|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|Antigua and Barbuda|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Argentina|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|            Armenia|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|Australian Capita...|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|     New South Wales|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|  Northern Territory|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|          Queensland|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|     South Australia|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|            Tasmania|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|            Victoria|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|          Australia|   Western Australia|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|            Austria|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|         Azerbaijan|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "|            Bahamas|                NULL|      0|      0|      0|      0|      0|      0|      0|      0|\n",
      "+-------------------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Количество строк: 289\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CSV Read Example\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.read.csv(\"/home/jovyan/data/raw_2020.csv\", header=True, inferSchema=True, sep=';')\n",
    "\n",
    "# df.printSchema()\n",
    "\n",
    "first_columns = df.columns[:10]\n",
    "df.select(first_columns).orderBy('1/29/20', reverse=False).show()\n",
    "\n",
    "print(f\"Количество строк: {df.count()}\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6b8035-83b1-4ada-8467-02940f5e186f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+------------+\n",
      "| id|first_name|last_name|age|created_date|\n",
      "+---+----------+---------+---+------------+\n",
      "|  2|     Мария|  Петрова| 25|  2023-02-20|\n",
      "|  1|      Иван|   Иванов| 30|  2023-01-15|\n",
      "|  3|   Алексей|  Сидоров| 35|  2023-03-10|\n",
      "+---+----------+---------+---+------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- created_date: date (nullable = true)\n",
      "\n",
      "Количество строк: 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "        .appName(\"PostgreSQL Read Example\") \n",
    "        .config(\"spark.jars\", \"/path/to/postgresql-42.5.0.jar\") \n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# table = 'raw.users'\n",
    "table = \"ab_permission\"\n",
    "\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/airflow\"\n",
    "postgres_properties = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .jdbc(url=postgres_url, \n",
    "              table=table, \n",
    "              properties=postgres_properties)\n",
    ")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "print(f\"Количество строк: {df.count()}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1fe7d8-c7cc-4c5f-bcac-6a373514ab76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные для вставки:\n",
      "+---+----------+---------+---+------------+\n",
      "| id|first_name|last_name|age|created_date|\n",
      "+---+----------+---------+---+------------+\n",
      "|  1|      Иван|   Иванов| 30|  2023-01-15|\n",
      "|  2|     Мария|  Петрова| 25|  2023-02-20|\n",
      "|  3|   Алексей|  Сидоров| 35|  2023-03-10|\n",
      "+---+----------+---------+---+------------+\n",
      "\n",
      "Данные в базе:\n",
      "+---+----------+---------+---+------------+\n",
      "| id|first_name|last_name|age|created_date|\n",
      "+---+----------+---------+---+------------+\n",
      "|  2|     Мария|  Петрова| 25|  2023-02-20|\n",
      "|  1|      Иван|   Иванов| 30|  2023-01-15|\n",
      "|  3|   Алексей|  Сидоров| 35|  2023-03-10|\n",
      "+---+----------+---------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "        .appName(\"PostgreSQL Read Example\") \n",
    "        .config(\"spark.jars\", \"/path/to/postgresql-42.5.0.jar\") \n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (1, \"Иван\", \"Иванов\", 30, \"2023-01-15\"),\n",
    "    (2, \"Мария\", \"Петрова\", 25, \"2023-02-20\"),\n",
    "    (3, \"Алексей\", \"Сидоров\", 35, \"2023-03-10\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"created_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.withColumn(\"created_date\", to_date(\"created_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"Данные для вставки:\")\n",
    "df.show()\n",
    "\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/airflow\"\n",
    "postgres_properties = {\n",
    "    \"user\": \"airflow\",\n",
    "    \"password\": \"airflow\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "(df.write \n",
    "    .mode(\"append\")\n",
    "    .jdbc(url=postgres_url, \n",
    "          table=\"raw.users\", \n",
    "          properties=postgres_properties)\n",
    ")\n",
    "\n",
    "print(\"Данные в базе:\")\n",
    "(\n",
    "    spark.read\n",
    "        .jdbc(url=postgres_url, \n",
    "              table='raw.users', \n",
    "              properties=postgres_properties)\n",
    ").show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
